# Paths
SNITCH_LLVM_PATH = "/usr/bin"
SNITCH_SW_PATH = "/opt/snax-kul-cluster-mixed-narrow-wide"


# Binaries
VLTSIM = "/opt/snax-kul-cluster-mixed-narrow-wide-rtl/bin/snitch_cluster.vlt"
CC = f"{SNITCH_LLVM_PATH}/clang"
LD = CC
MLIROPT = "mlir-opt"
MLIRTRANSLATE = "mlir-translate"
SNAXOPT = "snax-opt"

# Flags
CFLAGS = [
    "-Wno-unused-command-line-argument",
    f"-I{SNITCH_SW_PATH}/target/snitch_cluster/sw/runtime/rtl-generic/src",
    f"-I{SNITCH_SW_PATH}/target/snitch_cluster/sw/runtime/common",
    f"-I{SNITCH_SW_PATH}/sw/snRuntime/api",
    f"-I{SNITCH_SW_PATH}/sw/snRuntime/src",
    f"-I{SNITCH_SW_PATH}/sw/snRuntime/src/omp/",
    f"-I{SNITCH_SW_PATH}/sw/snRuntime/api/omp/",
    f"-I{SNITCH_SW_PATH}/sw/math/arch/riscv64/bits/",
    f"-I{SNITCH_SW_PATH}/sw/math/arch/generic",
    f"-I{SNITCH_SW_PATH}/sw/math/src/include",
    f"-I{SNITCH_SW_PATH}/sw/math/src/internal",
    f"-I{SNITCH_SW_PATH}/sw/math/include/bits",
    f"-I{SNITCH_SW_PATH}/sw/math/include",
    f"-I../../runtime/include",
    "-D__DEFINED_uint64_t",
    "--target=riscv32-unknown-elf",
    "-mcpu=generic-rv32",
    "-march=rv32imafdzfh",
    "-mabi=ilp32d",
    "-mcmodel=medany",
    "-ftls-model=local-exec",
    "-ffast-math",
    "-fno-builtin-printf",
    "-fno-common",
    "-O3"
]
CFLAGS.extend(["-std=gnu11", "-Wall", "-Wextra"])

CFLAGS.append(f"-I{SNITCH_SW_PATH}/target/snitch_cluster/sw/snax/streamer-gemm/include")


LDFLAGS = [
    f"-fuse-ld={SNITCH_LLVM_PATH}/ld.lld",
    "--target=riscv32-unknown-elf",
    "-mcpu=generic-rv32",
    "-march=rv32imafdzfh",
    "-mabi=ilp32d",
    "-mcmodel=medany",
    f"-T{SNITCH_SW_PATH}/sw/snRuntime/base.ld",
    f"-L{SNITCH_SW_PATH}/target/snitch_cluster/sw/runtime/rtl-generic",
    f"-L{SNITCH_SW_PATH}/target/snitch_cluster/sw/runtime/rtl-generic/build",
    "-nostdlib",
    "-lsnRuntime"
]


MLIRPREPROCFLAGS = [
    "--linalg-generalize-named-ops",
    "--mlir-print-op-generic",
    "--mlir-print-local-scope"
]

MLIROPTFLAGS = [
    "--convert-linalg-to-loops",
    "--convert-scf-to-cf",
    "--lower-affine",
    "--canonicalize",
    "--cse",
    "--convert-math-to-llvm",
    "--llvm-request-c-wrappers",
    "--expand-strided-metadata",
    "--lower-affine",
    "--convert-index-to-llvm=index-bitwidth=32",
    "--convert-cf-to-llvm=index-bitwidth=32",
    "--convert-arith-to-llvm=index-bitwidth=32",
    "--convert-func-to-llvm='index-bitwidth=32'",
    "--finalize-memref-to-llvm='use-generic-functions index-bitwidth=32'",
    "--canonicalize",
    "--reconcile-unrealized-casts",
]

def get_snax_flags(layout, remove_copy):
    return  ','.join([
        "convert-linalg-to-kernel",
        "insert-accfg-op{accelerator=snax_gemmx}",
        "dispatch-kernels",
        "convert-linalg-to-stream",
        "fuse-streaming-regions",
        "stream-bufferize",
        "snax-bufferize",
        "alloc-to-global",
        "set-memory-space",
        f"set-memory-layout{{gemm_layout={layout}}}",
        "realize-memref-casts",
        "test-remove-memref-copy",
        "insert-sync-barrier",
        "dispatch-regions{nb_cores=3}",
        "convert-stream-to-snax-stream",
        "convert-linalg-to-accfg",
        "test-add-mcycle-around-launch",
        "convert-accfg-to-csr",
        "snax-copy-to-dma",
        "memref-to-snax",
        "snax-to-func",
        "snax-lower-mcycle",
        "clear-memory-space"])

# Rules

from gendata import create_test_data

rule generate_data:
    output:
        "data_{m}_{n}_{k}_{add_c}.c",
        "data_{m}_{n}_{k}_{add_c}.h"
    params:
        m = lambda wildcards : int(wildcards.m),
        n = lambda wildcards : int(wildcards.n),
        k = lambda wildcards : int(wildcards.k),
        add_c = lambda wildcards : True if wildcards.add_c == "gemm" else False,
    run:
        create_test_data(params.n, params.m, params.k, f"data_{wildcards.m}_{wildcards.n}_{wildcards.k}_{wildcards.add_c}", params.add_c)


rule postprocess_llvm_module:
    input:
        "{file}.ll"
    output:
        temp("{file}.ll12")
    shell:
        "../../runtime/tollvm12.py < {input} > {output} "

rule compile_llvm_module:
    input:
        "{file}.ll12"
    output:
        temp("{file}.o")
    shell:
	    "{CC} {CFLAGS} -x ir -c {input} -o {output}"

rule render_main:
    input:
        "main.c"
    output:
        temp("main_{m}_{n}_{k}_{add_c}.rendered.c")
    shell:
        'echo \"#include \\"data_{wildcards.m}_{wildcards.n}_{wildcards.k}_{wildcards.add_c}.h\\"" | cat - {input} > {output}'

rule compile_main:
    input:
        "data_{m}_{n}_{k}_{add_c}.o",
        "main_{m}_{n}_{k}_{add_c}.rendered.c"
    output:
        temp("main_{m}_{n}_{k}_{add_c}.o")
    shell:
        "{CC} {CFLAGS} {input[0]} -c {input[1]} -o {output}"


rule compile_snax_binary:
    input:
        "generated_{m}_{n}_{k}_{add_c}_{layout}.o",
        "main_{m}_{n}_{k}_{add_c}.o",
        "data_{m}_{n}_{k}_{add_c}.o"
    output:
        "generated_{m}_{n}_{k}_{add_c}_{layout}.x"
    shell:
        "{LD} {LDFLAGS} {input} -o {output}"

rule compile_c:
    input:
        "{file}.c"
    output:
        temp("{file}.o")
    shell:
        "{CC} {CFLAGS} -c {input} -o {output}"


rule clean:
    shell:
        "rm -rf *.ll12 *.x *.o *.logs/ logs/ data.h data.c"

rule preprocess_mlir:
    input:
        "{file}.generated.mlir"
    output:
        temp("{file}.preprocfinal.mlir")
    run:
        shell("{MLIROPT} {MLIRPREPROCFLAGS} -o {wildcards.file}.preprocfinal.mlir {input}")

rule snax_opt_mlir:
    input:
        "generated_{m}_{n}_{k}_{add_c}.preprocfinal.mlir"
    output:
        temp("generated_{m}_{n}_{k}_{add_c}_{layout}.snax-opt.mlir")
    params:
        snax_flags = lambda wildcards : get_snax_flags(wildcards.layout, True)
    shell:
        "{SNAXOPT} -p {params.snax_flags} -o {output} {input}"

rule translate_mlir:
    input:
        "{file}.ll.mlir"
    output:
        temp("{file}.ll")
    shell:
        "{MLIRTRANSLATE} --mlir-to-llvmir -o {output} {input}"

rule postprocess_mlir:
    input:
        "{file}.snax-opt.mlir"
    output:
        temp("{file}.ll.mlir")
    shell:
        "{MLIROPT} {MLIROPTFLAGS} -o {output} {input}"

from genbenchmark import create_matrix_multiply, write_module_to_file

rule generate_mlir:
    output:
        "generated_{m}_{n}_{k}_{algo}.generated.mlir"
    params:
        m = lambda wildcards: int(wildcards.m),
        n = lambda wildcards: int(wildcards.n),
        k = lambda wildcards: int(wildcards.k),
        add_c = lambda wildcards: True if wildcards.algo == "gemm" else False
    run:
        write_module_to_file(create_matrix_multiply(params.k, params.m, params.n, params.add_c), output[0])

import itertools

def get_sizes():
    selected_dims = [32, 48, 64]
    sizes = list(itertools.product(selected_dims, repeat=3))

    # some other relevant neural network sizes:
    nn_size = [
        # m, n, k
        # tiled small matrix sizes from LSTM
        [16, 32, 512],
        # tiled small matrix sizes from MobileNetV2
        [448, 32, 32],
        [8, 192, 32],
        [8, 16, 16],
        [224, 16, 192],
        [8, 96, 16],
        [64, 24, 96],
        [8, 48, 24],
        [56, 48, 16],
        [8, 32, 144],
        [56, 32, 32],
        [200, 48, 16],
        [200, 32, 64],
        [200, 96, 16],
        [200, 8, 384],
        [200, 8, 96],
        [56, 576, 16],
        [8, 160, 576],
        [56, 48, 160],
        [8, 960, 16],
        [56, 64, 960],
        [56, 64, 320],
        [8, 40, 1280],
        # tiled small matrix sizes from ResNet18
        [8, 32, 152],
        [8, 64, 576],
        [8, 128, 576],
        [112, 128, 128],
        [56, 32, 64],
        [40, 64, 1152],
        [200, 64, 192],
        [200, 32, 128],
        [56, 8, 576],
        [56, 8, 512],
        [56, 128, 256],
        [8, 200, 512],
        # tiled small matrix sizes from Vision-Transformer
        [40, 96, 768],
        [40, 200, 64],
        [200, 64, 200],
        [40, 8, 768],
        [8, 128, 192],
        [8, 40, 768],
        # tiled small matrix sizes from I-BERTBase
        [32, 64, 768],
        [8, 512, 64],
        [32, 64, 512],
        [128, 8, 768],
        [128, 8, 792],
        [128, 88, 192],
    ]
    return sizes + nn_size

sizes = get_sizes()
add_c = ("gemm")
layout = ("banked","cyclic")

rule run_benchmarks:
    input:
        expand("generated_{size[0]}_{size[1]}_{size[2]}_{algo}_{layout}.x", size=sizes, algo=add_c, layout=layout)
